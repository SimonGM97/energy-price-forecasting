{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257bad88",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6ba7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "import holidays\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Feature Selectionb\n",
    "from boruta import BorutaPy\n",
    "import tsfresh as tsf\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Non-deep learning ML models\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Hyper-parameter tunning\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK\n",
    "from hyperopt.fmin import generate_trials_to_calculate\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a21ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a11a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30e844",
   "metadata": {},
   "source": [
    "# Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0339296",
   "metadata": {},
   "source": [
    "### Load Datasets\n",
    "- The **energy file** records Spain’s hourly electricity prices, in Euros per mega watt hour (€/MWh); electricity generation by type of origin (coal, gas, wind power, etc.) in MWh; and energy demand (“load”) in MWh.\n",
    "- The **weather file** offers the hourly records of five major Spanish cities.\n",
    "- We will aim to forecast **one lag ahead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7495fd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/energy_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Energy Dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m energy_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m energy_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Weather Features\u001b[39;00m\n\u001b[1;32m      6\u001b[0m weather_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/ITBA/Implementación de Aplicaciones de Aprendizaje Automático en la Nube/energy-price-forecasting/.price_forecasting_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/ITBA/Implementación de Aplicaciones de Aprendizaje Automático en la Nube/energy-price-forecasting/.price_forecasting_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/ITBA/Implementación de Aplicaciones de Aprendizaje Automático en la Nube/energy-price-forecasting/.price_forecasting_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/ITBA/Implementación de Aplicaciones de Aprendizaje Automático en la Nube/energy-price-forecasting/.price_forecasting_env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/ITBA/Implementación de Aplicaciones de Aprendizaje Automático en la Nube/energy-price-forecasting/.price_forecasting_env/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/energy_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Energy Dataset\n",
    "energy_path = os.path.join(\"datasets\", \"energy_dataset.csv\")\n",
    "energy_df = pd.read_csv(energy_path, header=0, parse_dates=[\"time\"])\n",
    "\n",
    "# Weather Features\n",
    "weather_path = os.path.join(\"datasets\", \"weather_features.csv\")\n",
    "weather_df = pd.read_csv(weather_path, header=0, parse_dates=[\"dt_iso\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad33d7",
   "metadata": {},
   "source": [
    "### Energy Data - Data Analysis & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa4ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare idx\n",
    "energy_df['time'] = pd.to_datetime(energy_df['time'], utc=True, infer_datetime_format=True).dt.tz_localize(None)\n",
    "energy_df.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b662d",
   "metadata": {},
   "source": [
    "#### Basic Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b24952",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3677ca",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "- We observe two empty feature columns. On the right-hand side, we notice a few gaps in individual rows of the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(energy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7e6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with all null values\n",
    "energy_df.dropna(axis=1, how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3857c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values from remaining rows\n",
    "energy_df = energy_df.interpolate(method =\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7b1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with all values equal to 0\n",
    "energy_df = energy_df.loc[:, (energy_df!=0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba9fab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"forecast\" columns\n",
    "energy_df = energy_df.drop(energy_df.filter(regex=\"forecast\").columns, axis=1, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6a764",
   "metadata": {},
   "source": [
    "#### Rename columns\n",
    "- The original column names contain some spaces and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d5db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'generation biomass': 'gen_bio', \n",
    "    'generation fossil brown coal/lignite': 'gen_lig', \n",
    "    'generation fossil coal-derived gas': 'gen_coal_gas', \n",
    "    'generation fossil gas': 'gen_gas', \n",
    "    'generation fossil hard coal': 'gen_coal', \n",
    "    'generation fossil oil': 'gen_oil', \n",
    "    'generation fossil oil shale': 'gen_oil_shale', \n",
    "    'generation fossil peat': 'gen_peat', \n",
    "    'generation geothermal': 'gen_geo', \n",
    "    'generation hydro pumped storage consumption': 'gen_hyd_pump', \n",
    "    'generation hydro run-of-river and poundage': 'gen_hyd_river', \n",
    "    'generation hydro water reservoir': 'gen_hyd_res', \n",
    "    'generation marine': 'gen_mar', \n",
    "    'generation nuclear': 'gen_nuc', \n",
    "    'generation other': 'gen_other', \n",
    "    'generation other renewable': 'gen_oth_renew', \n",
    "    'generation solar': 'gen_sol', \n",
    "    'generation waste': 'gen_waste', \n",
    "    'generation wind offshore': 'gen_wind_off', \n",
    "    'generation wind onshore': 'gen_wind_on', \n",
    "    'total load actual': 'load_actual', \n",
    "    'price day ahead': 'price_dayahead', \n",
    "    'price actual': 'price'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a1118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326b582",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23081329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicated idxs\n",
    "energy_df.loc[energy_df.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9b67459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows\n",
    "energy_df = energy_df.loc[~energy_df.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c163699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "energy_df = energy_df.loc[:, ~energy_df.columns.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c788b",
   "metadata": {},
   "source": [
    "#### Extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c143ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute z_scores\n",
    "z_scores = np.abs(st.zscore(energy_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b27545",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9857b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove values where z_score is over 2.5 stdev\n",
    "for col in energy_df.columns:\n",
    "    energy_df.loc[z_scores[col] > 2.5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e915ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with mean/median values for that column\n",
    "means_dict = {col: energy_df[col].median() for col in energy_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ed8275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.fillna(value=means_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2149f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76062b5",
   "metadata": {},
   "source": [
    "### Target Anlysis\n",
    "- A lineplot of the hourly prices shows that the curve that does not appear to follow a stable long-term trend or a single seasonality pattern, but fluctuates around a mean price of €60. \n",
    "- Since we deal with hourly prices, there might also be seasonalities related to **hours**, **weekdays** & **months**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e995742",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(100, figsize=(20, 7))\n",
    "sns.lineplot(\n",
    "    x=\"time\", \n",
    "    y=\"price\", \n",
    "    data=energy_df, \n",
    "    palette=\"coolwarm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1df2e8",
   "metadata": {},
   "source": [
    "#### Auto-Correlations & Partial Auto-Correlations\n",
    "- Both plots show statistically significant AC values and PAC values\n",
    "- This indicates that the data is autocorrelated, which could imply that there are some seasonality effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(100, figsize=(20, 7))\n",
    "plot_acf(energy_df['price'], lags=20, alpha=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(energy_df['price'], lags=20, alpha=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adbfd9",
   "metadata": {},
   "source": [
    "#### Seasonal decomposition\n",
    "- Decomposing the time series with both **weekly** and **monthly** frequencies portray a decisive seasonality component for the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1d65714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data to plot\n",
    "last_year = energy_df.loc[energy_df.index > '2018', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1679e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Weekly frequency\n",
    "decomp = seasonal_decompose(last_year, period=24*7)\n",
    "\n",
    "decomp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Monthly frequency\n",
    "decomp = seasonal_decompose(last_year, period=24*30)\n",
    "\n",
    "decomp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7d0f6",
   "metadata": {},
   "source": [
    "### Weather Data - Data Analysis & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74314864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare idx\n",
    "weather_df['dt_iso'] = pd.to_datetime(weather_df['dt_iso'], utc=True, infer_datetime_format=True).dt.tz_localize(None)\n",
    "weather_df.set_index('dt_iso', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8a109",
   "metadata": {},
   "source": [
    "#### Basic Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.city_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c23717",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3615f8",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "- No missing values were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e89ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with all values equal to 0\n",
    "weather_df = weather_df.loc[:, (weather_df!=0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4fc65",
   "metadata": {},
   "source": [
    "#### Manage Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bedbf7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "drop_cols = [\"rain_3h\", \"weather_id\", \"weather_main\", \"weather_description\", \"weather_icon\"]\n",
    "weather_df.drop(drop_cols, inplace=True, axis=1, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f052a04",
   "metadata": {},
   "source": [
    "#### Basic Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7128235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature: kelvin to celsius\n",
    "temp_cols = [col for col in weather_df.columns if \"temp\" in col]\n",
    "weather_df[temp_cols] = weather_df[temp_cols].filter(like=\"temp\").applymap(lambda t: t - 273.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "629cfb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert int and float64 columns to float32\n",
    "intcols = list(weather_df.dtypes[weather_df.dtypes == np.int64].index)\n",
    "weather_df[intcols] = weather_df[intcols].applymap(np.float32)\n",
    "\n",
    "f64cols = list(weather_df.dtypes[weather_df.dtypes == np.float64].index)\n",
    "weather_df[f64cols] = weather_df[f64cols].applymap(np.float32)\n",
    "\n",
    "f32cols = list(weather_df.dtypes[weather_df.dtypes == np.float32].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ef17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25302c77",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c92eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows\n",
    "weather_df = (\n",
    "    weather_df\n",
    "    .reset_index()\n",
    "    .drop_duplicates(subset=[\"dt_iso\", \"city_name\"], keep=\"first\")\n",
    "    .set_index('dt_iso')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f4656e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "weather_df = weather_df.loc[:, ~weather_df.columns.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bad3c",
   "metadata": {},
   "source": [
    "#### Re-group DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "754a5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_df = weather_df.groupby(\"city_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35c4335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df: pd.DataFrame, city: str):\n",
    "    df.drop(columns=['city_name'], inplace=True)\n",
    "    return df.add_suffix(f\"_{city.replace(' ', '')}\")\n",
    "\n",
    "concat_weather_df = pd.concat(\n",
    "    [format_df(gb_df.get_group(city), city) for city in gb_df.groups.keys()],\n",
    "    axis=1\n",
    ").interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e82f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966cfba",
   "metadata": {},
   "source": [
    "#### Extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50742770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute z_scores\n",
    "num_cols = list(concat_weather_df.select_dtypes(include=['number']).columns)\n",
    "z_scores = np.abs(st.zscore(concat_weather_df[num_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eae42097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove values where z_score is over 2.5 stdev\n",
    "for col in num_cols:\n",
    "    concat_weather_df.loc[z_scores[col] > 2.5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "241ca94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with mean values for that column\n",
    "means_dict = {col: concat_weather_df[col].mean() for col in num_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b299db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_weather_df.fillna(value=means_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fcf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_weather_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95ae74",
   "metadata": {},
   "source": [
    "### Merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e798e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy & Weather Concat\n",
    "df = pd.concat([energy_df, concat_weather_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0d2c8",
   "metadata": {},
   "source": [
    "Define ***Target***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62e3dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f057a",
   "metadata": {},
   "source": [
    "Define ***Features***\n",
    "- Features will be *lagged* one observation, as we will be forecasting *one period ahead*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f34cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forecast = pd.DataFrame(\n",
    "    index=df.index[-1:] + pd.Timedelta(minutes=60),\n",
    "    columns=df.columns.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "081ea5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (\n",
    "    pd.concat([df, X_forecast])\n",
    "    .shift(1)\n",
    "    .fillna(method='bfill')\n",
    "    .rename(columns={'price': 'lagged_price'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ea4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec723cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([y, X[['lagged_price']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c373492",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc4a38",
   "metadata": {},
   "source": [
    "### Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cafeec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Correlations\n",
    "df_corr = pd.concat([y, X.loc[X.index.isin(y.index)]], axis=1).corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a392685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold for features filtering\n",
    "thresh = np.max([np.quantile(np.abs(df_corr['price'].dropna()), 0.25), 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6697d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick top features and filter df_corr\n",
    "top_candidates = list(df_corr.loc[np.abs(df_corr['price']).sort_values() > thresh].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a537fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = (\n",
    "    df_corr\n",
    "    .loc[top_candidates, top_candidates]\n",
    "    .sort_values(by=['price'], ascending=False)\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "df_corr = df_corr[df_corr.index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada59d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (17,17))\n",
    "sns.set(font_scale=0.75)\n",
    "ax = sns.heatmap(\n",
    "    df_corr, \n",
    "    annot=True, \n",
    "    square=True, \n",
    "    linewidths=.75, \n",
    "    cmap=\"coolwarm\", \n",
    "    fmt = \".2f\", \n",
    "    annot_kws = {\"size\": 11}\n",
    ")\n",
    "ax.xaxis.tick_bottom()\n",
    "plt.title(\"correlation matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features distributions\n",
    "X = X.filter(top_candidates)\n",
    "\n",
    "rows = int(np.ceil(X.shape[1] / 4))\n",
    "f, ax = plt.subplots(rows, 4, figsize=(20, rows*4.5), gridspec_kw={'wspace':0.5,'hspace':0.3})\n",
    "\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, col in enumerate(X):\n",
    "    sns.histplot(X[col].astype(float), ax=ax[i], kde=False)\n",
    "    ax[i].axvline(x=X[col].mean(), color='k', label='mean')\n",
    "    ax[i].axvline(x=X[col].median(), color='r', label='median')\n",
    "    \n",
    "ax[0].legend();\n",
    "ax[-2].axis('off')\n",
    "ax[-1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced5332",
   "metadata": {},
   "source": [
    "### Feature Enrichment\n",
    "- There are multiple strategies for enriching features to boost the achievable forecast accuracy of a model.\n",
    "- This might include techniques like binning, linear combinations of some key features, laggin features, calculating moving average, exponential moing averages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50c045ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_seasonal_period(time_series, max_period=None):\n",
    "    if max_period is None:\n",
    "        max_period = len(time_series) // 2\n",
    "\n",
    "    # Perform seasonal decomposition using STL\n",
    "    decomposition = seasonal_decompose(time_series, period=max_period)\n",
    "\n",
    "    # Get the seasonal component\n",
    "    seasonal_component = decomposition.seasonal.dropna()\n",
    "\n",
    "    # Compute the periodogram\n",
    "    n = len(seasonal_component)\n",
    "    fft_values = np.abs(np.fft.fft(seasonal_component)) ** 2\n",
    "    fft_values = fft_values[:n // 2]\n",
    "    frequencies = np.fft.fftfreq(n, 1)\n",
    "    frequencies = frequencies[:n // 2]\n",
    "\n",
    "    # Find the index of the maximum frequency\n",
    "    max_index = np.argmax(fft_values)\n",
    "    # pprint(fft_values)\n",
    "\n",
    "    # Calculate the optimal seasonal period\n",
    "    optimal_period = int(1 / frequencies[max_index])\n",
    "\n",
    "    # Plot the periodogram (optional)\n",
    "    plt.plot(1 / frequencies, fft_values)\n",
    "    plt.xlabel('Seasonal Period')\n",
    "    plt.ylabel('Periodogram')\n",
    "    plt.title('Periodogram of Seasonal Component')\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5fd70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = find_optimal_seasonal_period(y.copy(), 24*3*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34843df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_periods = list(range(1, sp//2 + 1)) + [sp * i for i in range(1, sp//2 + 1)] + [24*31, 24*365]\n",
    "rolling_windows = [sp//2, sp, 2*sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98daa6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lag_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e71b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagg Features\n",
    "def lag_df(df_: pd.DataFrame, lag: int):\n",
    "    df_[df_.columns] = df_[df_.columns].shift(lag, axis=0)\n",
    "    \n",
    "    return (\n",
    "        df_\n",
    "        .rename(columns=lambda x: f\"{x}_lag_{lag}\" if x in df_.columns else x)\n",
    "        .fillna(method='bfill')\n",
    "    )\n",
    "\n",
    "\n",
    "# Simple rolling features\n",
    "def rolling_df(df_: pd.DataFrame, window: int, agg_fun: str = 'mean'):\n",
    "    if agg_fun == 'mean':\n",
    "        df_[df_.columns] = df_.rolling(window=window).mean()\n",
    "    elif agg_fun == 'std':\n",
    "        df_[df_.columns] = df_.rolling(window=window).std()\n",
    "    elif agg_fun == 'max':\n",
    "        df_[df_.columns] = df_.rolling(window=window).max()\n",
    "    elif agg_fun == 'min':\n",
    "        df_[df_.columns] = df_.rolling(window=window).min()\n",
    "    elif agg_fun == 'min_max':\n",
    "        df_[df_.columns] = df_.rolling(window=window).max() - df_.rolling(window=window).min()\n",
    "        \n",
    "    return (\n",
    "        df_\n",
    "        .rename(columns=lambda x: f\"{x}_sm_{agg_fun}_{window}\" if x in df_.columns else x)\n",
    "        .fillna(method='bfill')\n",
    "    )\n",
    "\n",
    "\n",
    "# Exponential Moving Average\n",
    "def ema(df_: pd.DataFrame, window: int):\n",
    "    # ewm(span=window, adjust=False)\n",
    "    df_[df_.columns] = df_.ewm(span=window, adjust=False).mean()\n",
    "    \n",
    "    return (\n",
    "        df_\n",
    "        .rename(columns=lambda x: f\"{x}_ema_{window}\" if x in df_.columns else x)\n",
    "        .fillna(method='bfill')\n",
    "    )\n",
    "\n",
    "\n",
    "# Temporal Embedding Features\n",
    "def tef(df_: pd.DataFrame, dow: bool = True, dom: bool = True, hod: bool = True):\n",
    "    # Day of Week\n",
    "    if dow:\n",
    "        df_['dow_sin'] = np.sin(2 * np.pi * df_.index.dayofweek / 7)\n",
    "        df_['day_cos'] = np.cos(2 * np.pi * df_.index.dayofweek / 7)\n",
    "    \n",
    "    # Day of Month\n",
    "    if dom:\n",
    "        df_['dom_sin'] = np.sin(2 * np.pi * df_.index.day / 31)\n",
    "        df_['dam_cos'] = np.cos(2 * np.pi * df_.index.day / 31)\n",
    "        \n",
    "    # Hour of Day\n",
    "    if hod:\n",
    "        df_['hod_sin'] = np.sin(2 * np.pi * df_.index.hour / 24)\n",
    "        df_['hod_cos'] = np.cos(2 * np.pi * df_.index.hour / 24)\n",
    "        \n",
    "    return pd.concat([\n",
    "        df_.filter(like='sin', axis=1), \n",
    "        df_.filter(like='cos', axis=1)\n",
    "    ], axis=1)\n",
    "\n",
    "\n",
    "# Time Based Features\n",
    "def tbf(df_: pd.DataFrame):\n",
    "    # Time-based Features\n",
    "    df_['month'] = df_.index.month\n",
    "    df_['day'] = df_.index.day\n",
    "    df_['day_of_week'] = df_.index.dayofweek\n",
    "    df_['hour'] = df_.index.hour\n",
    "    \n",
    "    return df_[['month', 'day', 'day_of_week', 'hour']]\n",
    "\n",
    "\n",
    "# Holiday-based features\n",
    "def extract_holidays(df_: pd.DataFrame):\n",
    "    spain_holidays = holidays.CountryHoliday('ES', observed=True)\n",
    "    df_['is_holiday'] = df_.index.to_series().apply(lambda x: x.date() in spain_holidays)\n",
    "    \n",
    "    return df_[['is_holiday']]\n",
    "\n",
    "\n",
    "# Seasonal Decomposition\n",
    "def extract_stl(target: pd.DataFrame, seasonal_period: int):\n",
    "    if seasonal_period % 2 == 0:\n",
    "        seasonal_period += 1\n",
    "        \n",
    "    stl = STL(target.values, period=seasonal_period)\n",
    "    result = stl.fit()\n",
    "    \n",
    "    stl_df = pd.concat(\n",
    "        [pd.Series(result.trend), pd.Series(result.seasonal), pd.Series(result.resid)], axis=1\n",
    "    ).rename(columns={\n",
    "        0: 'stl_trend',\n",
    "        1: 'stl_season',\n",
    "        2: 'stl_resid'\n",
    "    })\n",
    "    stl_df.index = target.index\n",
    "    \n",
    "    return stl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d45e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df = pd.concat(\n",
    "    [lag_df(X.copy(), lag=lag) for lag in lag_periods],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94398c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bc5aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_df = pd.concat(\n",
    "    [rolling_df(X.copy(), window=window, agg_fun='mean') for window in rolling_windows],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4892b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "129da111",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_std_df = pd.concat(\n",
    "    [rolling_df(X.copy(), window=window, agg_fun='std') for window in rolling_windows],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_std_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9055e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_max_df = pd.concat(\n",
    "    [rolling_df(X.copy(), window=window, agg_fun='max') for window in rolling_windows],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_max_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a800248",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_min_df = pd.concat(\n",
    "    [rolling_df(X.copy(), window=window, agg_fun='min') for window in rolling_windows],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841d36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_min_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efb1ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_min_max_df = pd.concat(\n",
    "    [rolling_df(X.copy(), window=window, agg_fun='min_max') for window in rolling_windows],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c58913",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_min_max_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e39a408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_df = pd.concat(\n",
    "    [ema(X.copy(), window=window) for window in rolling_windows],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f873f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92e21cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tef_df = tef(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fee426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tef_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "166eba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbf_df = tbf(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbf_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7c74e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = extract_holidays(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384778f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d73513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_df = extract_stl(target=X['lagged_price'], seasonal_period=sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64728e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c88d9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate extracted DataFrames\n",
    "X = pd.concat([\n",
    "    X, lag_df, sma_df, sm_std_df, sm_max_df, sm_min_df, \n",
    "    sm_min_max_df, ema_df, tef_df, tbf_df, holiday_df, stl_df\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873af000",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84ffcc",
   "metadata": {},
   "source": [
    "### Feature Standardizing\n",
    "- Since all features are numerical, OneHotEncoding will not be needed\n",
    "- We will use the StandardScaler to transform the numerical featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "011f76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "X_stand = pd.DataFrame(\n",
    "    scaler.fit_transform(X), \n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deefb74",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "- Similarly to the feature enrichment section, there are multiple ways to select the best subset of features that will aid our model, without overfitting.\n",
    "- We will start by filtering features based on their correlation with the **target**\n",
    "- Then, we will then filter out **colinear** features\n",
    "- Finally, we will leverage the **sklearn.feature_selection**, **boruta** and **tsfresh** libraries to select the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "58b2706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target-Feature Correlation Based Filtering\n",
    "def target_feature_correl_filter(\n",
    "    y: pd.Series,\n",
    "    X: pd.DataFrame,\n",
    "    q_thresh: float = 0.3,\n",
    "    debug: bool = False\n",
    "):\n",
    "    # Prepare DataFrames\n",
    "    intersection = y.index.intersection(X.index)\n",
    "    num_cols = list(X.select_dtypes(include=['number']).columns)\n",
    "\n",
    "    y = y.loc[intersection]\n",
    "    X = X.loc[intersection, num_cols]\n",
    "\n",
    "    # Calculate Correlations with target\n",
    "    tf_corr_df = pd.DataFrame(columns=[y.name])\n",
    "    for c in X.columns:\n",
    "        tf_corr_df.loc[c] = [abs(y.corr(X[c]))]\n",
    "    \n",
    "    tf_corr_df = tf_corr_df.sort_values(by=[y.name], ascending=False)\n",
    "    \n",
    "    if debug:\n",
    "        print(f'{tf_corr_df.head()}\\n')\n",
    "\n",
    "    # Define threshold\n",
    "    threshold = np.quantile(tf_corr_df[y.name].dropna(), q_thresh)\n",
    "    \n",
    "    return tf_corr_df.loc[tf_corr_df[y.name] > threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ecac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22034ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correl_filter = target_feature_correl_filter(\n",
    "    y=y.copy(),\n",
    "    X=X_stand.copy(),\n",
    "    q_thresh=0.3,\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c82dabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand = X_stand.loc[:, correl_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f898ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1043faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colinear_feature_filter(\n",
    "    X: pd.DataFrame,\n",
    "    thresh: float = 0.9\n",
    "):\n",
    "    cm = pd.DataFrame(X.corr().applymap(lambda x: 100 * np.abs(x))).fillna(100)\n",
    "    filtered_features = cm.columns.tolist()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(filtered_features):\n",
    "        keep_feature = filtered_features[i]\n",
    "        skip_features = cm.loc[\n",
    "            (cm[keep_feature] < 100) &\n",
    "            (cm[keep_feature] >= thresh*100)\n",
    "        ][keep_feature].index.tolist()\n",
    "        \n",
    "        if len(skip_features) > 0:\n",
    "            filtered_features = [c for c in filtered_features if c not in skip_features]\n",
    "        i += 1\n",
    "    \n",
    "    return filtered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "53322884",
   "metadata": {},
   "outputs": [],
   "source": [
    "colinear_filter = colinear_feature_filter(\n",
    "    X_stand.copy(), \n",
    "    thresh=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dcc09717",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand = X_stand.loc[:, colinear_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61658f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2f8db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best_features(\n",
    "    y: pd.Series,\n",
    "    X: pd.DataFrame,\n",
    "    perc: float = 0.1\n",
    "):\n",
    "    # Prepare DataFrames\n",
    "    intersection = y.index.intersection(X.index)\n",
    "    \n",
    "    binary_features = [col for col in X.columns if X[col].nunique() == 2]\n",
    "    non_binary_features = [col for col in X.columns if X[col].nunique() > 2]\n",
    "\n",
    "    y = y.loc[intersection]\n",
    "    X_binary = X.loc[intersection, binary_features]\n",
    "    X_non_binary = X.loc[intersection, non_binary_features]\n",
    "    \n",
    "    # Prepare selected_featuers\n",
    "    selected_featuers = []\n",
    "    \n",
    "    # Select Binary Features\n",
    "    if X_binary.shape[1] > 0:\n",
    "        k = int(perc * len(binary_features))\n",
    "        \n",
    "        selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "        selector.fit(X_binary, y)\n",
    "\n",
    "        selected_featuers.extend(X_binary.columns[selector.get_support()].tolist())\n",
    "    \n",
    "    # Select Non-Binary Features\n",
    "    if X_non_binary.shape[1] > 0:\n",
    "        k = int(perc * len(non_binary_features))\n",
    "        \n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        selector.fit(X_non_binary, y)\n",
    "\n",
    "        selected_featuers.extend(X_non_binary.columns[selector.get_support()].tolist())\n",
    "        \n",
    "    return selected_featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8599ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best_features = select_k_best_features(\n",
    "    y.copy(),\n",
    "    X_stand.copy(),\n",
    "    perc=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(k_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "33d7024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_boruta_features(\n",
    "    y: pd.Series,\n",
    "    X: pd.DataFrame,\n",
    "):\n",
    "    # Prepare DataFrames\n",
    "    intersection = y.index.intersection(X.index)\n",
    "\n",
    "    y = y.loc[intersection]\n",
    "    X = X.loc[intersection]\n",
    "    \n",
    "    # Choose the model\n",
    "    model = LGBMRegressor(num_iterations=100, random_state=23111997)\n",
    "\n",
    "    # Instanciate and fit the BorutaPy estimator\n",
    "    boruta = BorutaPy(model, n_estimators='auto', verbose=0, random_state=23111997)            \n",
    "    boruta.fit(\n",
    "        np.array(X), \n",
    "        np.array(y)\n",
    "    )\n",
    "\n",
    "    # Extract features\n",
    "    return X.columns[boruta.support_].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "64f6f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_features = select_boruta_features(\n",
    "    y.copy(),\n",
    "    X_stand.copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boruta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "84754edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tsfresh_features(\n",
    "    y: pd.Series,\n",
    "    X: pd.DataFrame,\n",
    "    p_value: float = 0.01,\n",
    "    top_k: int = None\n",
    "):\n",
    "    # Prepare DataFrames\n",
    "    intersection = y.index.intersection(X.index)\n",
    "\n",
    "    y = y.loc[intersection]\n",
    "    X = X.loc[intersection]\n",
    "    \n",
    "    # Run TSFresh Feature Selection\n",
    "    relevance_table = tsf.feature_selection.relevance.calculate_relevance_table(X, y)\n",
    "    relevance_table = relevance_table[relevance_table.relevant].sort_values(\"p_value\")\n",
    "    relevance_table = relevance_table.loc[relevance_table['p_value'] < p_value]\n",
    "    \n",
    "    selected_featuers = list(relevance_table[\"feature\"].values)\n",
    "    \n",
    "    # Extract Features\n",
    "    if top_k is not None:\n",
    "        return selected_featuers[:top_k]\n",
    "    return selected_featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "edd7e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh_features = select_tsfresh_features(\n",
    "    y.copy(),\n",
    "    X_stand.copy(),\n",
    "    p_value=0.01,\n",
    "    top_k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tsfresh_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "947b74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Features\n",
    "selected_features = list(set(k_best_features + boruta_features + tsfresh_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "65d7af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand = X_stand.loc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba551e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b740f",
   "metadata": {},
   "source": [
    "### Selected Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cd111888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.concat([y, X_stand.loc[X_stand.index.isin(y.index)]], axis=1).corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e1305e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = (\n",
    "    df_corr\n",
    "    .sort_values(by=['price'], ascending=False)\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "df_corr = df_corr[df_corr.index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (17,17))\n",
    "sns.set(font_scale=0.75)\n",
    "ax = sns.heatmap(\n",
    "    df_corr, \n",
    "    annot=True, \n",
    "    square=True, \n",
    "    linewidths=.75, cmap=\"coolwarm\", \n",
    "    fmt = \".2f\", \n",
    "    annot_kws = {\"size\": 11}\n",
    ")\n",
    "ax.xaxis.tick_bottom()\n",
    "plt.title(\"correlation matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8077d2",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d0360fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    save_attrs = [\n",
    "        'algorithm',\n",
    "        'hyper_parameters',\n",
    "        'fitted'\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        algorithm: str = None,\n",
    "        hyper_parameters: dict = None,\n",
    "    ) -> None:\n",
    "        self.model = None\n",
    "        self.algorithm = algorithm\n",
    "        self.hyper_parameters = None\n",
    "        if hyper_parameters is not None:\n",
    "            self.hyper_parameters = self.prepare_hyper_parameters(deepcopy(hyper_parameters))\n",
    "            \n",
    "        self.fitted = False\n",
    "        \n",
    "    def prepare_hyper_parameters(\n",
    "        self,\n",
    "        hyper_parameters: dict\n",
    "    ):\n",
    "        if self.algorithm == 'sarimax':\n",
    "            # Order\n",
    "            if 'order' not in hyper_parameters:\n",
    "                order = (\n",
    "                    hyper_parameters['p'], \n",
    "                    hyper_parameters['d'], \n",
    "                    hyper_parameters['q']\n",
    "                )\n",
    "            else:\n",
    "                order = hyper_parameters['order']\n",
    "            \n",
    "            # Seasonal Order\n",
    "            if 'seasonal_order' not in hyper_parameters:                \n",
    "                seasonal_order = (\n",
    "                    hyper_parameters['seasonal_P'], \n",
    "                    hyper_parameters['seasonal_D'],\n",
    "                    hyper_parameters['seasonal_Q'], \n",
    "                    hyper_parameters['seasonal_S']\n",
    "                )\n",
    "            else:\n",
    "                seasonal_order = hyper_parameters['seasonal_order']\n",
    "            \n",
    "            # Trend\n",
    "            if 'trend' not in hyper_parameters:\n",
    "                trend = hyper_parameters['sarimax.trend']\n",
    "            else:\n",
    "                trend = hyper_parameters['trend']\n",
    "            \n",
    "            hyper_parameters = {\n",
    "                'order': order,  # (p, d, q)\n",
    "                'seasonal_order': seasonal_order,  # (P, D, Q, S)\n",
    "                'trend': trend, \n",
    "                'measurement_error': False,\n",
    "                'time_varying_regression': False, \n",
    "                'mle_regression': True,\n",
    "                'simple_differencing': False,\n",
    "                'enforce_stationarity': False, \n",
    "                'enforce_invertibility': False,\n",
    "                'hamilton_representation': False, \n",
    "                'concentrate_scale': False,\n",
    "                'trend_offset': 1, \n",
    "                'use_exact_diffuse': False, \n",
    "                'dates': None\n",
    "            }\n",
    "            \n",
    "        if self.algorithm == 'random_forest':\n",
    "            names = list(hyper_parameters.keys()).copy()\n",
    "            for param_name in names:\n",
    "                if 'random_forest.' in param_name:\n",
    "                    correct_name = param_name.replace('random_forest.', '')\n",
    "                    hyper_parameters[correct_name] = hyper_parameters.pop(param_name)\n",
    "            \n",
    "            hyper_parameters.update(**{\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 23111997\n",
    "            })\n",
    "        \n",
    "        if self.algorithm == 'lightgbm':\n",
    "            names = list(hyper_parameters.keys()).copy()\n",
    "            for param_name in names:\n",
    "                if 'lightgbm.' in param_name:\n",
    "                    correct_name = param_name.replace('lightgbm.', '')\n",
    "                    hyper_parameters[correct_name] = hyper_parameters.pop(param_name)\n",
    "                \n",
    "            hyper_parameters.update(**{\n",
    "                \"objective\": 'regression',\n",
    "                \"importance_type\": 'split',\n",
    "                \"random_state\": 23111997,\n",
    "                \"n_jobs\": -1\n",
    "            })\n",
    "                \n",
    "        return hyper_parameters\n",
    "                \n",
    "    def build(\n",
    "        self,\n",
    "        train_target: pd.Series = None, \n",
    "        train_features: pd.DataFrame = None\n",
    "    ):\n",
    "        if self.algorithm == 'expo_smooth':\n",
    "            self.model = ExponentialSmoothing(\n",
    "                train_target,\n",
    "                trend=self.hyper_parameters['expo_smooth.trend'], \n",
    "                damped_trend=self.hyper_parameters['damped_trend'], \n",
    "                seasonal=self.hyper_parameters['seasonal'], \n",
    "                seasonal_periods=self.hyper_parameters['seasonal_periods']\n",
    "            )\n",
    "            \n",
    "        elif self.algorithm == 'sarimax':\n",
    "            if self.model is not None:\n",
    "                sarimax_parameters['start_ar_lags'] = self.model.ar_lags\n",
    "                sarimax_parameters['start_ma_lags'] = self.model.ma_lags\n",
    "            \n",
    "            self.model = SARIMAX(\n",
    "                endog=train_target.values.astype(float),\n",
    "                exog=train_features.values.astype(float),\n",
    "                **sarimax_parameters\n",
    "            )\n",
    "            \n",
    "        elif self.algorithm == 'random_forest':\n",
    "            self.model = RandomForestRegressor(**self.hyper_parameters)\n",
    "            \n",
    "        elif self.algorithm == 'lightgbm':\n",
    "            self.model = LGBMRegressor(**self.hyper_parameters)\n",
    "        \n",
    "        self.fitted = False\n",
    "            \n",
    "    def fit(\n",
    "        self,\n",
    "        train_target: pd.Series = None, \n",
    "        train_features: pd.DataFrame = None\n",
    "    ):\n",
    "        if self.algorithm == 'expo_smooth':\n",
    "            self.model.fit()\n",
    "            \n",
    "        elif self.algorithm == 'sarimax':\n",
    "            self.model = self.model.fit(\n",
    "                disp=False, \n",
    "                maxiter=50\n",
    "            )\n",
    "        \n",
    "        elif self.algorithm == 'random_forest':\n",
    "            self.model.fit(\n",
    "                train_features.values.astype(float), \n",
    "                train_target.values.astype(float)\n",
    "            )\n",
    "        \n",
    "        elif self.algorithm == 'lightgbm':\n",
    "            self.model.fit(\n",
    "                train_features.values.astype(float), \n",
    "                train_target.values.astype(float)\n",
    "            )\n",
    "            \n",
    "        self.fitted = True\n",
    "            \n",
    "    def predict(\n",
    "        self,\n",
    "        train_target: pd.Series = None,\n",
    "        forecast_features: pd.DataFrame = None, \n",
    "        forecast_dates: Iterable = None\n",
    "    ):\n",
    "        if self.algorithm == 'expo_smooth':\n",
    "            return self.model.predict(\n",
    "                self.model.params,\n",
    "                start=forecast_dates[0],\n",
    "                end=forecast_dates[-1],\n",
    "            )\n",
    "        \n",
    "        elif self.algorithm == 'sarimax':\n",
    "            return self.model.forecast(\n",
    "                steps=forecast_features.shape[0],\n",
    "                exog=forecast_features.values.astype(float)\n",
    "            )\n",
    "        \n",
    "        elif self.algorithm == 'random_forest':\n",
    "            return self.model.predict(\n",
    "                forecast_features.values.astype(float)\n",
    "            )\n",
    "        \n",
    "        elif self.algorithm == 'lightgbm':\n",
    "            return self.model.predict(\n",
    "                forecast_features.values.astype(float)\n",
    "            )\n",
    "        \n",
    "    def save(\n",
    "        self,\n",
    "        as_champion: bool = True\n",
    "    ):\n",
    "        save_attrs = {\n",
    "            attr_name: attr_value for attr_name, attr_value in self.__dict__.items()\n",
    "            if attr_name in self.save_attrs\n",
    "        }\n",
    "        \n",
    "        if as_champion:\n",
    "            print('Saving new champion.\\n')\n",
    "            \n",
    "            # Define paths\n",
    "            model_path = os.path.join('models', 'champion', 'champion.pickle')\n",
    "            attrs_path = os.path.join('models', 'champion', 'attrs.pickle')\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('Saving new challenger.\\n')\n",
    "            \n",
    "            # Define paths\n",
    "            model_path = os.path.join('models', 'challenger', 'challenger.pickle')\n",
    "            attrs_path = os.path.join('models', 'challenger', 'attrs.pickle')\n",
    "                \n",
    "        # Save self.model            \n",
    "        with open(model_path, 'wb') as file:\n",
    "            pickle.dump(self.model, file)\n",
    "\n",
    "        # Save attrs            \n",
    "        with open(attrs_path, 'wb') as file:\n",
    "            pickle.dump(save_attrs, file)\n",
    "            \n",
    "    def load(\n",
    "        self,\n",
    "        champion: bool = True\n",
    "    ):\n",
    "        if champion:\n",
    "            print('Loading champion.\\n')\n",
    "            \n",
    "            # Define paths\n",
    "            model_path = os.path.join('models', 'champion', 'champion.pickle')\n",
    "            attrs_path = os.path.join('models', 'champion', 'attrs.pickle')\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('Loading challenger.\\n')\n",
    "            \n",
    "            # Define paths\n",
    "            model_path = os.path.join('models', 'challenger', 'challenger.pickle')\n",
    "            attrs_path = os.path.join('models', 'challenger', 'attrs.pickle')\n",
    "                \n",
    "        # Load self.model            \n",
    "        with open(model_path, 'rb') as file:\n",
    "            self.model = pickle.load(file)\n",
    "\n",
    "        # Load attrs            \n",
    "        with open(attrs_path, 'rb') as file:\n",
    "            attrs = pickle.load(file)\n",
    "            \n",
    "        for attr_name, attr_value in attrs.items():\n",
    "            setattr(self, attr_name, attr_value)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e18d54",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2ca79240",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_periods = int(y.shape[0] * 0.8)\n",
    "test_periods = int(y.shape[0] * 0.2)\n",
    "\n",
    "X_train, X_test = X_stand.iloc[:train_periods], X_stand.iloc[train_periods:train_periods+test_periods]\n",
    "y_train, y_test = y.iloc[:train_periods], y.iloc[train_periods:train_periods+test_periods]\n",
    "X_forecast = X_stand.iloc[train_periods+test_periods:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "55331126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "algorithms = [\n",
    "    # 'expo_smooth', \n",
    "    # 'sarimax',     \n",
    "    'random_forest',\n",
    "    'lightgbm',\n",
    "    # 'lstm',\n",
    "]\n",
    "\n",
    "int_parameters = [\n",
    "    # expo_smooth\n",
    "    'seasonal_periods',\n",
    "    \n",
    "    # sarimax\n",
    "    'p', 'd', 'q',    \n",
    "    'seasonal_P',\n",
    "    'seasonal_D',\n",
    "    'seasonal_Q',\n",
    "    'seasonal_S',\n",
    "    \n",
    "    # random_forest\n",
    "    'random_forest.n_estimators',\n",
    "    'random_forest.max_depth',\n",
    "    'random_forest.min_samples_split',\n",
    "    \n",
    "    # lightgbm\n",
    "    'lightgbm.n_estimators',\n",
    "    'lightgbm.max_depth',\n",
    "    'lightgbm.min_child_samples',\n",
    "    'num_leaves',\n",
    "    \n",
    "    # lstm\n",
    "    'layers',\n",
    "    'units',\n",
    "    'batch_size',\n",
    "]\n",
    "\n",
    "choice_parameters = {\n",
    "    # expo_smooth\n",
    "    \"expo_smooth.trend\": ['additive', 'multiplicative', None],\n",
    "    \"damped_trend\": [True, False],\n",
    "    \"seasonal\": ['additive', 'multiplicative', None],\n",
    "    \"seasonal_periods\": [12, 24, 24*7, 24*30],\n",
    "    \n",
    "    # sarimax\n",
    "    \"sarimax.trend\": [None, 'ct'],\n",
    "    \"seasonal_S\": [12, 24], # , 24*7, 24*30],\n",
    "    \n",
    "    # random_forest\n",
    "    \"random_forest.max_features\": [1.0, 'sqrt'],\n",
    "    \n",
    "    # lightgbm\n",
    "    \"boosting_type\": ['gbdt', 'dart'],\n",
    "    \n",
    "    # lstm\n",
    "    \"topology\": ['classic', 'bidirectional', 'convolutional'],\n",
    "    \"units\": [16, 32, 64, 128, 256],\n",
    "    \"batch_size\": [16, 32, 64, 128, 256]\n",
    "}\n",
    "\n",
    "model_type_choices = [\n",
    "    # Exponential Smoothing Search Space\n",
    "#     {\n",
    "#         \"algorithm\": 'expo_smooth',\n",
    "#         \"expo_smooth.trend\": hp.choice('expo_smooth.trend', choice_parameters['expo_smooth.trend']),\n",
    "#         \"damped_trend\": hp.choice('damped_trend', choice_parameters['damped_trend']),\n",
    "#         \"seasonal\": hp.choice('seasonal', choice_parameters['seasonal']),\n",
    "#         \"seasonal_periods\": scope.int(hp.choice('seasonal_periods', choice_parameters['seasonal_periods']))\n",
    "#     },\n",
    "    \n",
    "    # SARIMAX Search Space\n",
    "#     {\n",
    "#         \"algorithm\": 'sarimax',\n",
    "#         \"sarimax.trend\": hp.choice('sarimax.trend', choice_parameters['sarimax.trend']),\n",
    "#         \"p\": scope.int(hp.quniform('p', 0, 6, 1)),\n",
    "#         # \"d\": scope.int(hp.quniform('d', 0, 1, 1)),\n",
    "#         \"q\": scope.int(hp.quniform('q', 0, 3, 1)),\n",
    "#         \"seasonal_P\": scope.int(hp.quniform('seasonal_P', 0, 3, 1)),\n",
    "#         # \"seasonal_D\": scope.int(hp.quniform('seasonal_D', 0, 1, 1)),\n",
    "#         \"seasonal_Q\": scope.int(hp.quniform('seasonal_Q', 0, 2, 1)),\n",
    "#         \"seasonal_S\": scope.int(hp.choice('seasonal_S', choice_parameters['seasonal_S']))\n",
    "#     },\n",
    "    \n",
    "    # Random Forest Search Space\n",
    "    {\n",
    "        \"algorithm\": 'random_forest',\n",
    "        \"random_forest.n_estimators\": scope.int(hp.quniform('random_forest.n_estimators', 5, 200, 1)),\n",
    "        \"random_forest.max_depth\": scope.int(hp.quniform('random_forest.max_depth', 1, 100, 1)),\n",
    "        \"random_forest.min_samples_split\": scope.int(hp.quniform('random_forest.min_samples_split', 5, 100, 1)),\n",
    "        \"random_forest.max_features\": hp.choice('random_forest.max_features', choice_parameters['random_forest.max_features']),\n",
    "    },\n",
    "    \n",
    "    # LightGBM Search Space\n",
    "    {\n",
    "        \"algorithm\": 'lightgbm',\n",
    "        \"boosting_type\": hp.choice('boosting_type', choice_parameters['boosting_type']),\n",
    "        \"lightgbm.n_estimators\": scope.int(hp.quniform('lightgbm.n_estimators', 5, 350, 1)),\n",
    "        \"lightgbm.max_depth\": scope.int(hp.quniform('lightgbm.max_depth', 1, 175, 1)),\n",
    "        \"lightgbm.min_child_samples\": scope.int(hp.quniform('lightgbm.min_child_samples', 5, 100, 1)),\n",
    "        \"lightgbm.learning_rate\": hp.loguniform('lightgbm.learning_rate', np.log(0.001), np.log(0.3)),\n",
    "        \"num_leaves\": scope.int(hp.quniform('num_leaves', 5, 150, 1)),        \n",
    "        \"colsample_bytree\": hp.uniform('colsample_bytree', 0.5, 1)\n",
    "    },\n",
    "    \n",
    "    # LSTM Search Space\n",
    "    # {\n",
    "    #     \"algorithm\": 'lstm',\n",
    "    #     # \"topology\": hp.choice('topology', choice_parameters['topology']),\n",
    "    #     \"layers\": scope.int(hp.quniform('layers', 1, 10, 1)), \n",
    "    #     \"units\": scope.int(hp.choice('units', choice_parameters['units'])),\n",
    "    #     \"dropout\": hp.uniform('dropout', 0.0, 0.5),\n",
    "    #     \"recurrent_dropout\": hp.uniform('recurrent_dropout', 0.0, 0.5),\n",
    "    #     \"lstm.learning_rate\": hp.loguniform('lstm.learning_rate', np.log(0.001), np.log(0.1)),\n",
    "    #     # \"batch_size\": scope.int(hp.choice('batch_size', choice_parameters['batch_size'])),\n",
    "    # },\n",
    "]\n",
    "\n",
    "search_space = {\n",
    "    \"model_type\": hp.choice('model_type', model_type_choices)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1bbd25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_params(params: dict):\n",
    "    # SARIMAX\n",
    "    if params['model_type']['algorithm'] == 'sarimax':\n",
    "        if 'd' not in params['model_type']:\n",
    "            params['model_type']['d'] = 0\n",
    "            \n",
    "        if 'seasonal_D' not in params['model_type']:\n",
    "            params['model_type']['seasonal_D'] = 0\n",
    "            \n",
    "        if params['model_type']['seasonal_P'] == params['model_type']['seasonal_Q'] == 0:\n",
    "            params['model_type']['seasonal_S'] = 0\n",
    "            \n",
    "        if params['model_type']['q'] == params['model_type']['seasonal_Q']:\n",
    "            if params['model_type']['q'] > 0:\n",
    "                params['model_type']['q'] += 1\n",
    "                \n",
    "        if params['model_type']['p'] == params['model_type']['seasonal_P']:\n",
    "            params['model_type']['p'] += 1\n",
    "    \n",
    "    # Exponential Smoothing\n",
    "    if params['model_type']['algorithm'] == 'expo_smooth':\n",
    "        if params['model_type']['expo_smooth.trend'] is None:\n",
    "            params['model_type']['damped_trend'] = False\n",
    "            \n",
    "    return params\n",
    "\n",
    "\n",
    "def find_trials():\n",
    "    def extract_param_idx(name, value):\n",
    "        if name in choice_parameters:\n",
    "            return choice_parameters[name].index(value)\n",
    "        return value\n",
    "    \n",
    "    def find_trials_dict(champion: bool = True):        \n",
    "        # Load Champion\n",
    "        model = Model()\n",
    "        model.load(champion=champion)\n",
    "\n",
    "        # Define dict\n",
    "        trials_dict = {\n",
    "            'model_type': algorithms.index(model.algorithm)\n",
    "        }\n",
    "        trials_dict.update({\n",
    "            k: extract_param_idx(k, v) for k, v in model.hyper_parameters.items()\n",
    "        })\n",
    "        \n",
    "        return trials_dict\n",
    "    \n",
    "    trials_list = [find_trials_dict(champ) for champ in [True, False]]\n",
    "    \n",
    "    return generate_trials_to_calculate(trials_list)\n",
    "\n",
    "\n",
    "def objective(params: dict):\n",
    "    trials.append(params)\n",
    "    # try:\n",
    "    # Extract algorithm\n",
    "    algorithm = params['model_type']['algorithm']\n",
    "\n",
    "    # Fix params\n",
    "    params = fix_params(params)\n",
    "\n",
    "    # Extract Hyper Parameters\n",
    "    hyper_parameters = {\n",
    "        k: v for k, v in params['model_type'].items() if k != 'algorithm'\n",
    "    }\n",
    "\n",
    "    ts_split = TimeSeriesSplit(n_splits=3, test_size=test_periods)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in ts_split.split(X_train):\n",
    "        # Split Features\n",
    "        train_features = X_train.iloc[train_idx]\n",
    "        val_features = X_train.iloc[val_idx]\n",
    "\n",
    "        # Split Target\n",
    "        train_target = y_train.iloc[train_idx]\n",
    "        val_target = y_train.iloc[val_idx]\n",
    "\n",
    "        # Instanciate the Model\n",
    "        model = Model(\n",
    "            algorithm=algorithm,\n",
    "            hyper_parameters=hyper_parameters,\n",
    "        )\n",
    "\n",
    "        # Build the model\n",
    "        model.build(\n",
    "            train_target=train_target.copy(), \n",
    "            train_features=train_features.copy()\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(\n",
    "            train_target=train_target.copy(), \n",
    "            train_features=train_features.copy()\n",
    "        )\n",
    "\n",
    "        # Predict the Validation Dataset\n",
    "        val_pred = model.predict(\n",
    "            train_target=train_target.copy(), \n",
    "            forecast_features=val_features.copy(), \n",
    "            forecast_dates=val_features.index\n",
    "        )\n",
    "\n",
    "        scores.append(mean_absolute_percentage_error(val_target, val_pred))\n",
    "\n",
    "    return {'loss': np.mean(scores), 'status': STATUS_OK}\n",
    "#     except Exception as e:\n",
    "#         logger.warning(f\"Unable to run {params}.\\n\"\n",
    "#                        f\"Exception {e}\")\n",
    "#         return {'loss': np.inf, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da29b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fdfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = []\n",
    "\n",
    "result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    # trials=find_trials(),\n",
    "    max_evals=300,\n",
    "    timeout=10 * 60, # 30 mins\n",
    "    verbose=True,\n",
    "    show_progressbar=True,\n",
    "    early_stop_fn=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba37b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [t['model_type']['algorithm'] for t in trials]\n",
    "counts = Counter(algos)\n",
    "\n",
    "for algo, reps in counts.items():\n",
    "    print(f\"{algo}: {round(reps * 100 / len(algos), 1)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1925e",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "da4661cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the \"challenger\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5e170e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_challenger(results: dict):\n",
    "    def extract_parameter(k, v):\n",
    "        if k in choice_parameters:\n",
    "            v = choice_parameters[k][v]\n",
    "        if k in int_parameters:\n",
    "            v = int(v)\n",
    "        return v\n",
    "    \n",
    "    algorithm = algorithms[results['model_type']]\n",
    "    \n",
    "    hyper_parameters = {\n",
    "        k: extract_parameter(k, v) for k, v in results.items() if k not in ['algorithm', 'model_type']\n",
    "    }\n",
    "    \n",
    "    return Model(\n",
    "        algorithm=algorithm,\n",
    "        hyper_parameters=hyper_parameters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "210c6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger = extract_challenger(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fea5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ea3a3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & Re-fit model\n",
    "challenger.build(\n",
    "    train_target=y_train, \n",
    "    train_features=X_train\n",
    ")\n",
    "\n",
    "challenger.fit(\n",
    "    train_target=y_train, \n",
    "    train_features=X_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Challenger\n",
    "challenger.save(as_champion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c2023e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Test Data\n",
    "test_pred = challenger.predict(\n",
    "    train_target=y_train, \n",
    "    forecast_features=X_test, \n",
    "    forecast_dates=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "20a6ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Performance (on unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f87ea742",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_mape = mean_absolute_percentage_error(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94238c",
   "metadata": {},
   "source": [
    "### Champion Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac12f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('models', 'champion', 'champion.pickle')):\n",
    "    # Load Champion\n",
    "    champion = Model()\n",
    "    champion.load(champion=True)\n",
    "    \n",
    "    # Fit champion\n",
    "    champion.fit(\n",
    "        train_target=y_train, \n",
    "        train_features=X_train\n",
    "    )\n",
    "    \n",
    "    # Forecast test data\n",
    "    test_pred = champion.predict(\n",
    "        train_target=y_train, \n",
    "        forecast_features=X_test, \n",
    "        forecast_dates=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Evaluate champion\n",
    "    champion_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "    \n",
    "    print(f'champion_mape: {round(champion_mape*100, 2)} %\\n'\n",
    "          f'challenger_mape: {round(challenger_mape*100, 2)} %\\n')\n",
    "    \n",
    "    if challenger_mape < champion_mape:\n",
    "        challenger.save(as_champion=True)\n",
    "else:\n",
    "    challenger.save(as_champion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe18f5-1850-4562-a152-e7a30c39cc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
